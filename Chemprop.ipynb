{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "chemprop_path = Path(\".\") / \"chemprop\"\n",
    "os.chdir(chemprop_path)\n",
    "\n",
    "import numpy as np\n",
    "from rdkit import RDLogger\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from lightning import pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "\n",
    "from chemprop import data, featurizers, models, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for AChE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /cache/arthurcerveira/MPNN-MT/checkpoints/AChE exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 42/42 [00:01<00:00, 25.47it/s, train_loss_step=0.225, val_loss=0.306, train_loss_epoch=0.189] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 42/42 [00:01<00:00, 25.26it/s, train_loss_step=0.225, val_loss=0.306, train_loss_epoch=0.189]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /cache/arthurcerveira/MPNN-MT/checkpoints/AChE/best-epoch=16-val_loss=0.30.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /cache/arthurcerveira/MPNN-MT/checkpoints/AChE/best-epoch=16-val_loss=0.30.ckpt\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 12/12 [00:00<00:00, 36.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6329134106636047     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7740098237991333     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6329134106636047    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7740098237991333    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test/mse': 0.7740098237991333, 'test/mae': 0.6329134106636047}]\n",
      "Training model for D2R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /cache/arthurcerveira/MPNN-MT/checkpoints/D2R exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 84/84 [00:03<00:00, 25.82it/s, train_loss_step=0.171, val_loss=0.378, train_loss_epoch=0.235]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 84/84 [00:03<00:00, 25.59it/s, train_loss_step=0.171, val_loss=0.378, train_loss_epoch=0.235]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /cache/arthurcerveira/MPNN-MT/checkpoints/D2R/best-epoch=19-val_loss=0.38.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /cache/arthurcerveira/MPNN-MT/checkpoints/D2R/best-epoch=19-val_loss=0.38.ckpt\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 35.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5599697232246399     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5661064386367798     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5599697232246399    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5661064386367798    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test/mse': 0.5661064386367798, 'test/mae': 0.5599697232246399}]\n",
      "Training model for D3R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /cache/arthurcerveira/MPNN-MT/checkpoints/D3R exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 35/35 [00:01<00:00, 26.55it/s, train_loss_step=0.273, val_loss=0.475, train_loss_epoch=0.198]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 35/35 [00:01<00:00, 26.21it/s, train_loss_step=0.273, val_loss=0.475, train_loss_epoch=0.198]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /cache/arthurcerveira/MPNN-MT/checkpoints/D3R/best-epoch=14-val_loss=0.45.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /cache/arthurcerveira/MPNN-MT/checkpoints/D3R/best-epoch=14-val_loss=0.45.ckpt\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 35.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6453957557678223     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7118385434150696     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6453957557678223    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7118385434150696    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test/mse': 0.7118385434150696, 'test/mae': 0.6453957557678223}]\n",
      "Training model for _5HT2A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /cache/arthurcerveira/MPNN-MT/checkpoints/_5HT2A exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 44/44 [00:01<00:00, 28.39it/s, train_loss_step=0.0587, val_loss=0.424, train_loss_epoch=0.275]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 44/44 [00:01<00:00, 28.07it/s, train_loss_step=0.0587, val_loss=0.424, train_loss_epoch=0.275]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /cache/arthurcerveira/MPNN-MT/checkpoints/_5HT2A/best-epoch=17-val_loss=0.42.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /cache/arthurcerveira/MPNN-MT/checkpoints/_5HT2A/best-epoch=17-val_loss=0.42.ckpt\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 12/12 [00:00<00:00, 37.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6171982288360596     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6624202728271484     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6171982288360596    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6624202728271484    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test/mse': 0.6624202728271484, 'test/mae': 0.6171982288360596}]\n",
      "Training model for MAOB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /cache/arthurcerveira/MPNN-MT/checkpoints/MAOB exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 26/26 [00:00<00:00, 32.57it/s, train_loss_step=0.369, val_loss=0.541, train_loss_epoch=0.275] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 26/26 [00:00<00:00, 32.06it/s, train_loss_step=0.369, val_loss=0.541, train_loss_epoch=0.275]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /cache/arthurcerveira/MPNN-MT/checkpoints/MAOB/best-epoch=16-val_loss=0.50.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /cache/arthurcerveira/MPNN-MT/checkpoints/MAOB/best-epoch=16-val_loss=0.50.ckpt\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 44.74it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7204970717430115     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8887014985084534     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7204970717430115    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8887014985084534    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test/mse': 0.8887014985084534, 'test/mae': 0.7204970717430115}]\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://chemprop.readthedocs.io/en/latest/training.html\n",
    "input_path = \"../data/Assays-pXC50/{target}.csv\" # path to your data .csv file\n",
    "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "smiles_column = 'SMILES' # name of the column containing SMILES strings\n",
    "target_columns = ['pXC50'] # list of names of the columns containing targets\n",
    "targets = [\"AChE\", \"D2R\", \"D3R\", \"_5HT2A\", \"MAOB\"]\n",
    "\n",
    "TRAIN = True\n",
    "\n",
    "if TRAIN:\n",
    "    for target in targets:\n",
    "        print(f\"Training model for {target}\")\n",
    "        df_input = pd.read_csv(input_path.format(target=target))\n",
    "        smis = df_input.loc[:, smiles_column].values\n",
    "        ys = df_input.loc[:, target_columns].values\n",
    "        splits = df_input.loc[:, \"split\"].values\n",
    "        all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smis, ys)]\n",
    "\n",
    "        mols = [d.mol for d in all_data]  # RDkit Mol objects are use for structure based splits\n",
    "        \n",
    "        # Get indices for train, val, and test from splits column\n",
    "        train_indices, val_indices, test_indices = (\n",
    "            np.array(np.where(splits == \"train\")), \n",
    "            np.array(np.where(splits == \"val\")),\n",
    "            np.array(np.where(splits == \"test\"))\n",
    "        )\n",
    "        \n",
    "        train_data, val_data, test_data = data.split_data_by_indices(\n",
    "            all_data, train_indices, val_indices, test_indices\n",
    "        )\n",
    "\n",
    "        featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "        train_dset = data.MoleculeDataset(train_data[0], featurizer)\n",
    "        scaler = train_dset.normalize_targets()\n",
    "\n",
    "        val_dset = data.MoleculeDataset(val_data[0], featurizer)\n",
    "        val_dset.normalize_targets(scaler)\n",
    "\n",
    "        test_dset = data.MoleculeDataset(test_data[0], featurizer)\n",
    "\n",
    "        train_loader = data.build_dataloader(train_dset, num_workers=num_workers)\n",
    "        val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
    "        test_loader = data.build_dataloader(test_dset, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "        mp = nn.BondMessagePassing()\n",
    "        agg = nn.MeanAggregation()\n",
    "        output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "        ffn = nn.RegressionFFN(output_transform=output_transform)\n",
    "        batch_norm = True\n",
    "\n",
    "        metric_list = [nn.metrics.MSE(), nn.metrics.MAE()] # , nn.metrics.R2Score()] # Only the first metric is used for training and early stopping\n",
    "        mpnn = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "\n",
    "        # Configure model checkpointing\n",
    "        checkpointing = ModelCheckpoint(\n",
    "            f\"../checkpoints/{target}\",  # Directory where model checkpoints will be saved\n",
    "            \"best-{epoch}-{val_loss:.2f}\",  # Filename format for checkpoints, including epoch and validation loss\n",
    "            \"val_loss\",  # Metric used to select the best checkpoint (based on validation loss)\n",
    "            mode=\"min\",  # Save the checkpoint with the lowest validation loss (minimization objective)\n",
    "            save_last=True,  # Always save the most recent checkpoint, even if it's not the best\n",
    "            enable_version_counter=False\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            logger=False,\n",
    "            enable_checkpointing=True, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "            enable_progress_bar=True,\n",
    "            accelerator=\"auto\",\n",
    "            devices=1,\n",
    "            max_epochs=20, # number of epochs to train for\n",
    "            callbacks=[checkpointing], # Use the configured checkpoint callback\n",
    "        )\n",
    "\n",
    "        trainer.fit(mpnn, train_loader, val_loader)\n",
    "        results = trainer.test(dataloaders=test_loader)\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating AChE - pXC50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 12/12 [00:00<00:00, 38.11it/s]\n",
      "0\n",
      "Mean Squared Error: 0.7809265835244809\n",
      "R2 Score: 0.6444298317718669\n",
      "Mean Absolute Error: 0.6370649826808609\n",
      "\n",
      "Evaluating D2R - pXC50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 36.35it/s]\n",
      "0\n",
      "Mean Squared Error: 0.5661063647845489\n",
      "R2 Score: 0.5604960482204995\n",
      "Mean Absolute Error: 0.55996967456258\n",
      "\n",
      "Evaluating D3R - pXC50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 39.07it/s]\n",
      "0\n",
      "Mean Squared Error: 0.7005197744556219\n",
      "R2 Score: 0.5589170066868601\n",
      "Mean Absolute Error: 0.6386870237333011\n",
      "\n",
      "Evaluating _5HT2A - pXC50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 12/12 [00:00<00:00, 39.54it/s]\n",
      "0\n",
      "Mean Squared Error: 0.6631354193855045\n",
      "R2 Score: 0.5542842896255107\n",
      "Mean Absolute Error: 0.6188471763604864\n",
      "\n",
      "Evaluating MAOB - pXC50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 47.57it/s] \n",
      "0\n",
      "Mean Squared Error: 0.8941769350421328\n",
      "R2 Score: 0.5860250536719389\n",
      "Mean Absolute Error: 0.7292846486737211\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>r2</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AChE</th>\n",
       "      <td>0.780927</td>\n",
       "      <td>0.644430</td>\n",
       "      <td>0.637065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2R</th>\n",
       "      <td>0.566106</td>\n",
       "      <td>0.560496</td>\n",
       "      <td>0.559970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3R</th>\n",
       "      <td>0.700520</td>\n",
       "      <td>0.558917</td>\n",
       "      <td>0.638687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_5HT2A</th>\n",
       "      <td>0.663135</td>\n",
       "      <td>0.554284</td>\n",
       "      <td>0.618847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAOB</th>\n",
       "      <td>0.894177</td>\n",
       "      <td>0.586025</td>\n",
       "      <td>0.729285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mse        r2       mae\n",
       "AChE    0.780927  0.644430  0.637065\n",
       "D2R     0.566106  0.560496  0.559970\n",
       "D3R     0.700520  0.558917  0.638687\n",
       "_5HT2A  0.663135  0.554284  0.618847\n",
       "MAOB    0.894177  0.586025  0.729285"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference: https://chemprop.readthedocs.io/en/latest/predicting.html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import torch\n",
    "from lightning import pytorch as pl\n",
    "from chemprop import data, featurizers, models\n",
    "\n",
    "\n",
    "targets = ['AChE', 'D2R', 'D3R', '_5HT2A', 'MAOB']\n",
    "mpnn_metrics = dict()\n",
    "\n",
    "def run_mpnn_on_smiles(smiles_input, mpnn):\n",
    "    test_data = [data.MoleculeDatapoint.from_smi(smi) for smi in smiles_input]\n",
    "\n",
    "    featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "    test_dset = data.MoleculeDataset(test_data, featurizer=featurizer)\n",
    "    test_loader = data.build_dataloader(test_dset, shuffle=False)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        trainer = pl.Trainer(\n",
    "            logger=False,\n",
    "            # enable_progress_bar=True,\n",
    "            accelerator=\"cuda\",\n",
    "            devices=1,\n",
    "        )\n",
    "        test_preds = trainer.predict(mpnn, test_loader)\n",
    "        test_preds = np.concatenate(test_preds, axis=0)  # Concatenate batches\n",
    "\n",
    "    # Check if the model is single or multi-target\n",
    "    tasks = test_preds.shape[1]\n",
    "    if tasks == 1:\n",
    "        # Single target model: return a 1D array (# of samples,)\n",
    "        return test_preds[:, 0]\n",
    "    \n",
    "    # Multi-target model: return a 2D array (# of targets, # of samples)\n",
    "    # Can be decomposed into multiple 1D arrays\n",
    "    preds_tasks = np.array([test_preds[:, i] for i in range(tasks)])\n",
    "    return preds_tasks\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"Evaluating {target} - pXC50\")\n",
    "    df_test = pd.read_csv(f\"../data/Assays-pXC50/{target}.csv\").query(\"split == 'test'\")\n",
    "    mpnn = models.MPNN.load_from_checkpoint(f'../checkpoints/{target}/last.ckpt')\n",
    "    \n",
    "    smiles_input = df_test[\"SMILES\"].tolist()\n",
    "    test_preds = run_mpnn_on_smiles(smiles_input, mpnn)\n",
    "\n",
    "    # Skip molecules that failed to be processed (prediction is nan)\n",
    "    df_test[\"pXC50_pred\"] = test_preds\n",
    "    print(df_test['pXC50_pred'].isna().sum())\n",
    "    df_test = df_test.dropna(subset=[\"pXC50_pred\"])\n",
    "    y_true = df_test[\"pXC50\"].values\n",
    "    test_preds = df_test[\"pXC50_pred\"].values\n",
    "\n",
    "    mse = mean_squared_error(y_true, test_preds)\n",
    "    r2 = r2_score(y_true, test_preds)\n",
    "    mae = mean_absolute_error(y_true, test_preds)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\\n\")\n",
    "    mpnn_metrics[target] = {\"mse\": mse, \"r2\": r2, \"mae\": mae}\n",
    "\n",
    "metrics_df = pd.DataFrame(mpnn_metrics).T\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ../data/Multitarget-pXC50/AChE-MAOB.csv\n",
      "Number of molecules in AChE: 3688\n",
      "Number of molecules in MAOB: 2232\n",
      "SMILES intersection: 65\n",
      "Number of molecules in the merged dataset: 5855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:02<00:00, 36.93it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:02<00:00, 38.11it/s]\n",
      "Creating ../data/Multitarget-pXC50/D2R-_5HT2A.csv\n",
      "Number of molecules in D2R: 7427\n",
      "Number of molecules in _5HT2A: 3826\n",
      "SMILES intersection: 1198\n",
      "Number of molecules in the merged dataset: 10055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 158/158 [00:04<00:00, 35.97it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 158/158 [00:04<00:00, 36.00it/s]\n",
      "Creating ../data/Multitarget-pXC50/D2R-D3R.csv\n",
      "Number of molecules in D2R: 7427\n",
      "Number of molecules in D3R: 3060\n",
      "SMILES intersection: 2615\n",
      "Number of molecules in the merged dataset: 7872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 123/123 [00:03<00:00, 35.73it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 123/123 [00:03<00:00, 35.08it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The multi-target datasets are created by concatenating the pXC50 values of the two targets.\n",
    "For each target combination, we will have a CSV file with the following columns:\n",
    "- SMILES, pXC50_{target1}, pXC50_{target2}, split\n",
    "The pXC50_{target1} and pXC50_{target2} columns are the pXC50 values of the two targets.\n",
    "To fill the missing values, we will predict the pXC50 values using the trained MPNN models.\n",
    "We must maintain the same train/val/test split for the multi-target datasets.\n",
    "\"\"\"\n",
    "target_combinations = (\n",
    "    # Alzheimers\n",
    "    (\"AChE\", \"MAOB\"),\n",
    "    # Schizophrenia\n",
    "    (\"D2R\", \"_5HT2A\"),\n",
    "    # Parkinsons\n",
    "    (\"D2R\", \"D3R\"),\n",
    ")\n",
    "\n",
    "CREATE_MULTITARGET_DATASETS = True\n",
    "\n",
    "if CREATE_MULTITARGET_DATASETS:\n",
    "    for targets in target_combinations:\n",
    "        output_csv = f\"../data/Multitarget-pXC50/{targets[0]}-{targets[1]}.csv\"\n",
    "        print(f\"Creating {output_csv}\")\n",
    "\n",
    "        # Load the datasets\n",
    "        df_target0 = pd.read_csv(f\"../data/Assays-pXC50/{targets[0]}.csv\")\n",
    "        df_target1 = pd.read_csv(f\"../data/Assays-pXC50/{targets[1]}.csv\")\n",
    "\n",
    "        print(f\"Number of molecules in {targets[0]}: {len(df_target0)}\")\n",
    "        print(f\"Number of molecules in {targets[1]}: {len(df_target1)}\")\n",
    "\n",
    "        print(\"SMILES intersection:\", len(set(df_target0[\"SMILES\"]).intersection(set(df_target1[\"SMILES\"]))))\n",
    "\n",
    "        # Merge the datasets\n",
    "        df = pd.merge(df_target0, df_target1, on=\"SMILES\", suffixes=(f\"_{targets[0]}\", f\"_{targets[1]}\"), how=\"outer\")\n",
    "\n",
    "        print(f\"Number of molecules in the merged dataset: {len(df)}\")\n",
    "\n",
    "        # Predict pXC50_{target0} using the MPNN model\n",
    "        mpnn_target0 = models.MPNN.load_from_checkpoint(f'../checkpoints/{targets[0]}/last.ckpt')\n",
    "        smiles_input = df[\"SMILES\"].tolist()\n",
    "        test_preds = run_mpnn_on_smiles(smiles_input, mpnn_target0)\n",
    "        preds_target0 = pd.Series(test_preds)\n",
    "        df[f\"pXC50_{targets[0]}\"] = df[f\"pXC50_{targets[0]}\"].fillna(preds_target0)\n",
    "\n",
    "        # Predict pXC50_{target1} using the MPNN model\n",
    "        mpnn_target1 = models.MPNN.load_from_checkpoint(f'../checkpoints/{targets[1]}/last.ckpt')\n",
    "        smiles_input = df[\"SMILES\"].tolist()\n",
    "        test_preds = run_mpnn_on_smiles(smiles_input, mpnn_target1)\n",
    "        preds_target1 = pd.Series(test_preds)\n",
    "        df[f\"pXC50_{targets[1]}\"] = df[f\"pXC50_{targets[1]}\"].fillna(preds_target1)\n",
    "\n",
    "        # Drop lines where pXC50_{target0} or pXC50_{target1} are missing\n",
    "        df = df.dropna(subset=[f\"pXC50_{targets[0]}\", f\"pXC50_{targets[1]}\"])\n",
    "\n",
    "        df[\"split\"] = df[f\"split_{targets[0]}\"].fillna(df[f\"split_{targets[1]}\"])\n",
    "        df[[\"SMILES\", f\"pXC50_{targets[0]}\", f\"pXC50_{targets[1]}\", \"split\"]].to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for ('AChE', 'MAOB')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /cache/arthurcerveira/MPNN-MT/checkpoints/AChE-MAOB exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/cache/arthurcerveira/MPNN-MT/chemprop/chemprop/nn/metrics.py:132: UserWarning: Using a target size (torch.Size([64, 2])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(preds, targets, reduction=\"none\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 66/66 [00:02<00:00, 30.97it/s, train_loss_step=0.864]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cache/arthurcerveira/MPNN-MT/chemprop/chemprop/nn/metrics.py:132: UserWarning: Using a target size (torch.Size([53, 2])) that is different to the input size (torch.Size([53, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(preds, targets, reduction=\"none\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   8%|▊         | 5/66 [00:00<00:01, 31.53it/s, train_loss_step=0.754, val_loss=0.999, train_loss_epoch=0.854] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cache/arthurcerveira/MPNN-MT/chemprop/chemprop/nn/metrics.py:132: UserWarning: Using a target size (torch.Size([20, 2])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(preds, targets, reduction=\"none\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 66/66 [00:02<00:00, 28.05it/s, train_loss_step=0.459, val_loss=0.677, train_loss_epoch=0.616]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 66/66 [00:02<00:00, 27.73it/s, train_loss_step=0.459, val_loss=0.677, train_loss_epoch=0.616]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /cache/arthurcerveira/MPNN-MT/checkpoints/AChE-MAOB/best-epoch=19-val_loss=0.68.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /cache/arthurcerveira/MPNN-MT/checkpoints/AChE-MAOB/best-epoch=19-val_loss=0.68.ckpt\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 19/19 [00:00<00:00, 37.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8170800805091858     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.071441411972046     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8170800805091858    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.071441411972046    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test/mse': 1.071441411972046, 'test/mae': 0.8170800805091858}]\n",
      "Training model for ('D2R', '_5HT2A')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /cache/arthurcerveira/MPNN-MT/checkpoints/D2R-_5HT2A exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/cache/arthurcerveira/MPNN-MT/chemprop/chemprop/nn/metrics.py:132: UserWarning: Using a target size (torch.Size([64, 2])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(preds, targets, reduction=\"none\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 114/114 [00:03<00:00, 30.03it/s, train_loss_step=1.120]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cache/arthurcerveira/MPNN-MT/chemprop/chemprop/nn/metrics.py:132: UserWarning: Using a target size (torch.Size([5, 2])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(preds, targets, reduction=\"none\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▍         | 5/114 [00:00<00:03, 30.31it/s, train_loss_step=0.725, val_loss=1.690, train_loss_epoch=0.870]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cache/arthurcerveira/MPNN-MT/chemprop/chemprop/nn/metrics.py:132: UserWarning: Using a target size (torch.Size([26, 2])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(preds, targets, reduction=\"none\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 114/114 [00:04<00:00, 27.16it/s, train_loss_step=0.317, val_loss=0.558, train_loss_epoch=0.523]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 114/114 [00:04<00:00, 26.99it/s, train_loss_step=0.317, val_loss=0.558, train_loss_epoch=0.523]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /cache/arthurcerveira/MPNN-MT/checkpoints/D2R-_5HT2A/best-epoch=19-val_loss=0.56.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /cache/arthurcerveira/MPNN-MT/checkpoints/D2R-_5HT2A/best-epoch=19-val_loss=0.56.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 32/32 [00:00<00:00, 35.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6115320920944214     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6428846716880798     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6115320920944214    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6428846716880798    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test/mse': 0.6428846716880798, 'test/mae': 0.6115320920944214}]\n",
      "Training model for ('D2R', 'D3R')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /cache/arthurcerveira/MPNN-MT/checkpoints/D2R-D3R exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/cache/arthurcerveira/MPNN-MT/chemprop/chemprop/nn/metrics.py:132: UserWarning: Using a target size (torch.Size([64, 2])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(preds, targets, reduction=\"none\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 89/89 [00:02<00:00, 30.12it/s, train_loss_step=0.664]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cache/arthurcerveira/MPNN-MT/chemprop/chemprop/nn/metrics.py:132: UserWarning: Using a target size (torch.Size([37, 2])) that is different to the input size (torch.Size([37, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(preds, targets, reduction=\"none\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▍         | 4/89 [00:00<00:02, 29.81it/s, train_loss_step=0.964, val_loss=1.290, train_loss_epoch=0.825] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cache/arthurcerveira/MPNN-MT/chemprop/chemprop/nn/metrics.py:132: UserWarning: Using a target size (torch.Size([56, 2])) that is different to the input size (torch.Size([56, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(preds, targets, reduction=\"none\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 89/89 [00:03<00:00, 26.36it/s, train_loss_step=0.656, val_loss=0.474, train_loss_epoch=0.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 89/89 [00:03<00:00, 26.13it/s, train_loss_step=0.656, val_loss=0.474, train_loss_epoch=0.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /cache/arthurcerveira/MPNN-MT/checkpoints/D2R-D3R/best-epoch=19-val_loss=0.47.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /cache/arthurcerveira/MPNN-MT/checkpoints/D2R-D3R/best-epoch=19-val_loss=0.47.ckpt\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 25/25 [00:00<00:00, 34.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5805091261863708     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5692845582962036     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5805091261863708    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5692845582962036    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test/mse': 0.5692845582962036, 'test/mae': 0.5805091261863708}]\n"
     ]
    }
   ],
   "source": [
    "# Train the multi-target MPNN models\n",
    "target_combinations = (\n",
    "    # Alzheimers\n",
    "    (\"AChE\", \"MAOB\"),\n",
    "    # Schizophrenia\n",
    "    (\"D2R\", \"_5HT2A\"),\n",
    "    # Parkinsons\n",
    "    (\"D2R\", \"D3R\"),\n",
    ")\n",
    "\n",
    "input_path = \"../data/Multitarget-pXC50/{target0}-{target1}.csv\" # path to your data .csv file\n",
    "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "smiles_column = 'SMILES' # name of the column containing SMILES strings\n",
    "target_columns = ['pXC50_{target0}', 'pXC50_{target1}'] # list of names of the columns containing targets\n",
    "\n",
    "TRAIN = True\n",
    "\n",
    "if TRAIN:\n",
    "    for target0, target1 in target_combinations:\n",
    "        print(f\"Training model for {(target0, target1)}\")\n",
    "        df_input = pd.read_csv(input_path.format(target0=target0, target1=target1))\n",
    "        smis = df_input.loc[:, smiles_column].values\n",
    "\n",
    "        multitarget_columns = [f\"pXC50_{target0}\", f\"pXC50_{target1}\"]\n",
    "        ys = df_input.loc[:, multitarget_columns].values\n",
    "        \n",
    "        splits = df_input.loc[:, \"split\"].values\n",
    "        all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smis, ys)]\n",
    "        mols = [d.mol for d in all_data]  # RDkit Mol objects are use for structure based splits\n",
    "        \n",
    "        # Get indices for train, val, and test from splits column\n",
    "        train_indices, val_indices, test_indices = (\n",
    "            np.array(np.where(splits == \"train\")), \n",
    "            np.array(np.where(splits == \"val\")),\n",
    "            np.array(np.where(splits == \"test\"))\n",
    "        )\n",
    "        \n",
    "        train_data, val_data, test_data = data.split_data_by_indices(\n",
    "            all_data, train_indices, val_indices, test_indices\n",
    "        )\n",
    "\n",
    "        featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "        train_dset = data.MoleculeDataset(train_data[0], featurizer)\n",
    "        scaler = train_dset.normalize_targets()\n",
    "\n",
    "        val_dset = data.MoleculeDataset(val_data[0], featurizer)\n",
    "        val_dset.normalize_targets(scaler)\n",
    "\n",
    "        test_dset = data.MoleculeDataset(test_data[0], featurizer)\n",
    "\n",
    "        train_loader = data.build_dataloader(train_dset, num_workers=num_workers)\n",
    "        val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
    "        test_loader = data.build_dataloader(test_dset, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "        mp = nn.BondMessagePassing()\n",
    "        agg = nn.MeanAggregation()\n",
    "        output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "        ffn = nn.RegressionFFN(output_transform=output_transform)\n",
    "        batch_norm = True\n",
    "\n",
    "        metric_list = [nn.metrics.MSE(), nn.metrics.MAE()] # , nn.metrics.R2Score()] # Only the first metric is used for training and early stopping\n",
    "        mpnn = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "\n",
    "        # Configure model checkpointing\n",
    "        checkpointing = ModelCheckpoint(\n",
    "            f\"../checkpoints/{target0}-{target1}\",  # Directory where model checkpoints will be saved\n",
    "            \"best-{epoch}-{val_loss:.2f}\",  # Filename format for checkpoints, including epoch and validation loss\n",
    "            \"val_loss\",  # Metric used to select the best checkpoint (based on validation loss)\n",
    "            mode=\"min\",  # Save the checkpoint with the lowest validation loss (minimization objective)\n",
    "            save_last=True,  # Always save the most recent checkpoint, even if it's not the best\n",
    "            enable_version_counter=False\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            logger=False,\n",
    "            enable_checkpointing=True, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
    "            enable_progress_bar=True,\n",
    "            accelerator=\"auto\",\n",
    "            devices=1,\n",
    "            max_epochs=20, # number of epochs to train for\n",
    "            callbacks=[checkpointing], # Use the configured checkpoint callback\n",
    "        )\n",
    "\n",
    "        trainer.fit(mpnn, train_loader, val_loader)\n",
    "        results = trainer.test(dataloaders=test_loader)\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ('AChE', 'MAOB') - pXC50\n",
      "- Evaluating on AChE dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 12/12 [00:00<00:00, 38.20it/s]\n",
      "Number of NaN predictions for AChE: 0\n",
      "* Mean Squared Error: 1.3606314024012394\n",
      "* R2 Score: 0.38047961632346816\n",
      "* Mean Absolute Error: 0.9378276905191107\n",
      "\n",
      "- Evaluating on MAOB dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 46.33it/s] \n",
      "Number of NaN predictions for MAOB: 0\n",
      "* Mean Squared Error: 1.429479629437975\n",
      "* R2 Score: 0.33819725192782035\n",
      "* Mean Absolute Error: 0.9150499585875782\n",
      "\n",
      "Evaluating ('D2R', '_5HT2A') - pXC50\n",
      "- Evaluating on D2R dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 36.70it/s]\n",
      "Number of NaN predictions for D2R: 0\n",
      "* Mean Squared Error: 0.8582009658345083\n",
      "* R2 Score: 0.33372465075745905\n",
      "* Mean Absolute Error: 0.7216015881402591\n",
      "\n",
      "- Evaluating on _5HT2A dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 12/12 [00:00<00:00, 39.94it/s]\n",
      "Number of NaN predictions for _5HT2A: 0\n",
      "* Mean Squared Error: 0.8211905388031981\n",
      "* R2 Score: 0.4480501061236515\n",
      "* Mean Absolute Error: 0.7043620530927773\n",
      "\n",
      "Evaluating ('D2R', 'D3R') - pXC50\n",
      "- Evaluating on D2R dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 37.81it/s]\n",
      "Number of NaN predictions for D2R: 0\n",
      "* Mean Squared Error: 0.7173887937507349\n",
      "* R2 Score: 0.4430459902428875\n",
      "* Mean Absolute Error: 0.6552236086348152\n",
      "\n",
      "- Evaluating on D3R dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/arthurcerveira/miniconda3/envs/chemprop/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 40.11it/s]\n",
      "Number of NaN predictions for D3R: 0\n",
      "* Mean Squared Error: 0.6073677154200291\n",
      "* R2 Score: 0.6175702960456524\n",
      "* Mean Absolute Error: 0.6080584795385872\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AChE': {'AChE_MAOB': {'mse': 1.3606314024012394,\n",
       "   'r2': 0.38047961632346816,\n",
       "   'mae': 0.9378276905191107}},\n",
       " 'MAOB': {'AChE_MAOB': {'mse': 1.429479629437975,\n",
       "   'r2': 0.33819725192782035,\n",
       "   'mae': 0.9150499585875782}},\n",
       " 'D2R': {'D2R__5HT2A': {'mse': 0.8582009658345083,\n",
       "   'r2': 0.33372465075745905,\n",
       "   'mae': 0.7216015881402591},\n",
       "  'D2R_D3R': {'mse': 0.7173887937507349,\n",
       "   'r2': 0.4430459902428875,\n",
       "   'mae': 0.6552236086348152}},\n",
       " '_5HT2A': {'D2R__5HT2A': {'mse': 0.8211905388031981,\n",
       "   'r2': 0.4480501061236515,\n",
       "   'mae': 0.7043620530927773}},\n",
       " 'D3R': {'D2R_D3R': {'mse': 0.6073677154200291,\n",
       "   'r2': 0.6175702960456524,\n",
       "   'mae': 0.6080584795385872}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import torch\n",
    "from lightning import pytorch as pl\n",
    "from chemprop import data, featurizers, models\n",
    "\n",
    "\n",
    "# Evaluate the multitarget MPNN models on the target-specific test sets\n",
    "target_combinations = (\n",
    "    # Alzheimers\n",
    "    (\"AChE\", \"MAOB\"),\n",
    "    # Schizophrenia\n",
    "    (\"D2R\", \"_5HT2A\"),\n",
    "    # Parkinsons\n",
    "    (\"D2R\", \"D3R\"),\n",
    ")\n",
    "\n",
    "targets = ['AChE', 'D2R', 'D3R', '_5HT2A', 'MAOB']\n",
    "multitarget_mpnn_metrics = dict()\n",
    "\n",
    "for target0, target1 in target_combinations:\n",
    "    print(f\"Evaluating {(target0, target1)} - pXC50\")\n",
    "    mpnn = models.MPNN.load_from_checkpoint(f'../checkpoints/{target0}-{target1}/last.ckpt')\n",
    "\n",
    "    for target in [target0, target1]:\n",
    "        print(f\"- Evaluating on {target} dataset\")\n",
    "        multitarget_mpnn_metrics.setdefault(target, dict())\n",
    "\n",
    "        df_test = pd.read_csv(f\"../data/Assays-pXC50/{target}.csv\").query(\"split == 'test'\")        \n",
    "        smiles_input = df_test[\"SMILES\"].tolist()\n",
    "        preds_t0, preds_t1 = run_mpnn_on_smiles(smiles_input, mpnn)\n",
    "\n",
    "        if target == target0:   test_preds = preds_t0\n",
    "        elif target == target1: test_preds = preds_t1\n",
    "\n",
    "        # Skip molecules that failed to be processed (prediction is nan)\n",
    "        df_test[\"pXC50_pred\"] = test_preds\n",
    "        nan_count = df_test['pXC50_pred'].isna().sum()\n",
    "        print(f\"Number of NaN predictions for {target}: {nan_count}\")\n",
    "        df_test = df_test.dropna(subset=[\"pXC50_pred\"])\n",
    "        y_true = df_test[\"pXC50\"].values\n",
    "        test_preds = df_test[\"pXC50_pred\"].values\n",
    "\n",
    "        mse = mean_squared_error(y_true, test_preds)\n",
    "        r2 = r2_score(y_true, test_preds)\n",
    "        mae = mean_absolute_error(y_true, test_preds)\n",
    "\n",
    "        print(f\"* Mean Squared Error: {mse}\")\n",
    "        print(f\"* R2 Score: {r2}\")\n",
    "        print(f\"* Mean Absolute Error: {mae}\\n\")\n",
    "        multitarget_mpnn_metrics[target][f\"{target0}_{target1}\"] = {\"mse\": mse, \"r2\": r2, \"mae\": mae}\n",
    "\n",
    "multitarget_mpnn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>r2</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">AChE</th>\n",
       "      <th>AChE_MAOB</th>\n",
       "      <td>1.36</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target-specific</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MAOB</th>\n",
       "      <th>AChE_MAOB</th>\n",
       "      <td>1.43</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target-specific</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">D2R</th>\n",
       "      <th>D2R__5HT2A</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2R_D3R</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target-specific</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">_5HT2A</th>\n",
       "      <th>D2R__5HT2A</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target-specific</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">D3R</th>\n",
       "      <th>D2R_D3R</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target-specific</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mse    r2   mae\n",
       "target model                            \n",
       "AChE   AChE_MAOB        1.36  0.38  0.94\n",
       "       target-specific  0.78  0.64  0.64\n",
       "MAOB   AChE_MAOB        1.43  0.34  0.92\n",
       "       target-specific  0.89  0.59  0.73\n",
       "D2R    D2R__5HT2A       0.86  0.33  0.72\n",
       "       D2R_D3R          0.72  0.44  0.66\n",
       "       target-specific  0.57  0.56  0.56\n",
       "_5HT2A D2R__5HT2A       0.82  0.45  0.70\n",
       "       target-specific  0.66  0.55  0.62\n",
       "D3R    D2R_D3R          0.61  0.62  0.61\n",
       "       target-specific  0.70  0.56  0.64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine with target-specific MPNN metrics\n",
    "for target in multitarget_mpnn_metrics:\n",
    "    multitarget_mpnn_metrics[target][\"target-specific\"] = mpnn_metrics[target]\n",
    "\n",
    "# Create multi-index DataFrame\n",
    "multi_index = pd.MultiIndex.from_tuples(\n",
    "    [(target, model) for target in multitarget_mpnn_metrics for model in multitarget_mpnn_metrics[target]],\n",
    "    names=[\"target\", \"model\"]\n",
    ")\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    [multitarget_mpnn_metrics[target][model] for target in multitarget_mpnn_metrics for model in multitarget_mpnn_metrics[target]],\n",
    "    index=multi_index\n",
    ")\n",
    "metrics_df.map(lambda x: round(x, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
